{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873bb9c9-5032-4d68-8163-b19a3ce09f4c",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First, load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c27d53-7f0b-4629-9c2d-ae84127512dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5499f2e7654bcaa700e16887a66f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487bebad058c4ae5b4076adbf54c83d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556a98daab6d4a0e9613df4f20508812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d182eeeef473450fbb9f231fe1528f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bd06e46b3f4f14a51f2057a7bf6472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a647cbc86c84417af336b1cd4efaa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_gpt2 import GPT2Model\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=\"hf_home/\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", cache_dir=\"hf_home/\")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da24a7-6bd4-44bc-9eb1-14e3c8b0cf72",
   "metadata": {},
   "source": [
    "### Initial Testing\n",
    "Now, let's do forward passes on some sample inputs. Starting with the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b5caef0-169b-4383-b8cf-6c416c20263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0502,  0.0018, -0.1750,  ..., -0.1020, -0.0257, -0.1292],\n",
       "         [-0.2410, -0.0911,  0.2592,  ...,  0.4394,  0.3465,  0.1077]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\").to('cuda')\n",
    "model.config._attn_implementation = \"sdpa\"  # NOTE: This is default, but we set manually here for emphasis.\n",
    "out = model.forward(inputs['input_ids'])\n",
    "out.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82643ead-2b81-40c8-b2dc-12148c8f5a17",
   "metadata": {},
   "source": [
    "Now, using our attention implementation. Default is `sdpa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deeaa06f-7e39-4723-9023-a108b3a551af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config._attn_implementation = \"minimal_attn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8428b93-1c78-4a6b-979d-4148903b8ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n",
      "default: torch.Size([1, 2, 12, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0502,  0.0018, -0.1750,  ..., -0.1020, -0.0257, -0.1292],\n",
       "         [-0.2410, -0.0911,  0.2592,  ...,  0.4394,  0.3465,  0.1077]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.forward(inputs['input_ids'])\n",
    "out.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ac93c-956d-4a9f-8a62-8b5162ef2800",
   "metadata": {},
   "source": [
    "Great! We can see the shapes are the same and the output tensors are too. This means the attention implementation is correct. Now, let's see if it is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ddd08-c092-40e7-b497-48d946bac2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1fa90c-2852-4118-a5c5-da0d69efbc85",
   "metadata": {},
   "source": [
    "### Generation\n",
    "Let's try generation now before starting the benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01dd95-a45c-494a-b88f-4db7ac66c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No generation for now bc some way the model works. Probably need to set up with AutoModel but can't figure out for now how to do that and retain our version of modeling_gpt2.py\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=5,\n",
    "#     return_dict_in_generate=True,\n",
    "#     output_scores=True,\n",
    "#     do_sample=False,  # temperature = 0.0 so deterministic\n",
    "# )\n",
    "\n",
    "# May need for batching\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
