{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873bb9c9-5032-4d68-8163-b19a3ce09f4c",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First, load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c27d53-7f0b-4629-9c2d-ae84127512dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_gpt2 import GPT2Model, GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=\"hf_home/\")\n",
    "# model = GPT2Model.from_pretrained(\"gpt2\", cache_dir=\"hf_home/\")  # for forward pass\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir=\"hf_home/\")  # for generation\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da24a7-6bd4-44bc-9eb1-14e3c8b0cf72",
   "metadata": {},
   "source": [
    "### Initial Testing for Correctness\n",
    "Now, let's do forward passes on some sample inputs. Starting with the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5caef0-169b-4383-b8cf-6c416c20263c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -36.3292,  -36.3402,  -40.4228,  ...,  -46.0234,  -44.5284,\n",
       "           -37.1276],\n",
       "         [-122.8355, -122.5403, -127.6362,  ..., -133.4906, -131.9769,\n",
       "          -125.4615]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\").to('cuda')\n",
    "model.transformer.config._attn_implementation = \"sdpa\"  # NOTE: This is default, but we set manually here for emphasis.\n",
    "out = model.forward(inputs['input_ids'])\n",
    "out.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82643ead-2b81-40c8-b2dc-12148c8f5a17",
   "metadata": {},
   "source": [
    "Now, using the minimal-flash-attn default attention implementation. Default is `sdpa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3817759b-a326-4f31-ba55-1d90e0f88657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8428b93-1c78-4a6b-979d-4148903b8ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -74.4814,  -71.7976,  -76.4575,  ...,  -80.6830,  -83.4880,\n",
       "           -74.0728],\n",
       "         [-163.7170, -161.1058, -166.5746,  ..., -173.9947, -176.1363,\n",
       "          -167.2979]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\").to('cuda')\n",
    "model.transformer.config._attn_implementation = \"mha_forward\"\n",
    "out = model.forward(inputs['input_ids'])\n",
    "out.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eadc1b-84be-4bc4-b12d-e14bd0713340",
   "metadata": {},
   "source": [
    "*Note*: These logits are different -- there's likely some problem with the gpu memory. If I run this multiple times, gpu memory goes up without getting cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc535ee-dbb6-43f9-8076-207ff90cc4a9",
   "metadata": {},
   "source": [
    "Now using our improved version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cefed803-f922-433e-8bc6-679bbf762056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -86.9953,  -82.0640,  -87.1137,  ...,  -99.9608,  -96.6098,\n",
       "           -86.1456],\n",
       "         [ -92.3755,  -88.2984,  -97.4849,  ..., -101.8799, -106.0120,\n",
       "           -90.7995]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\").to('cuda')\n",
    "model.transformer.config._attn_implementation = \"improved_mha_forward\"\n",
    "out = model.forward(inputs['input_ids'])\n",
    "out.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ac93c-956d-4a9f-8a62-8b5162ef2800",
   "metadata": {},
   "source": [
    "### Initial Testing for Timing\n",
    "Great! We can see the shapes are the same and the output tensors are too. This means the attention implementation is correct. Now, let's see if it is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c9d44e4-c793-4ceb-9696-4650961287e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"traces\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af3ddd08-c092-40e7-b497-48d946bac2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== profiling `sdpa` attention === \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      aten::addmm        11.60%       3.083ms        17.52%       4.659ms      97.066us       4.360ms        16.05%       5.354ms     111.542us            48  \n",
      "                                 aten::layer_norm         0.97%     258.963us        13.13%       3.491ms     139.639us     391.000us         1.44%       3.624ms     144.960us            25  \n",
      "                                 aten::contiguous         1.07%     284.328us        12.43%       3.305ms      91.796us     424.000us         1.56%       3.451ms      95.861us            36  \n",
      "                          aten::native_layer_norm         6.40%       1.701ms        11.77%       3.130ms     125.185us       1.983ms         7.30%       3.233ms     129.320us            25  \n",
      "               aten::scaled_dot_product_attention         0.92%     244.367us        11.34%       3.014ms     251.192us     297.000us         1.09%       3.068ms     255.667us            12  \n",
      "                                      aten::clone         3.12%     829.385us        10.74%       2.854ms      79.287us     983.000us         3.62%       3.027ms      84.083us            36  \n",
      "                                      aten::empty         6.58%       1.749ms         7.86%       2.089ms       9.947us       3.005ms        11.06%       3.005ms      14.310us           210  \n",
      "    aten::_scaled_dot_product_efficient_attention         2.64%     701.335us        10.21%       2.715ms     226.249us     749.000us         2.76%       2.771ms     230.917us            12  \n",
      "                                       aten::view         5.41%       1.438ms         5.41%       1.438ms       7.153us       2.425ms         8.93%       2.425ms      12.065us           201  \n",
      "                                  aten::transpose         4.04%       1.074ms         6.18%       1.643ms      16.942us       1.507ms         5.55%       2.086ms      21.505us            97  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 26.589ms\n",
      "Self CUDA time total: 27.164ms\n",
      "\n",
      "Total time taken: 0.04220151901245117\n",
      "\n",
      "\n",
      "=== profiling `mha_forward` attention === \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  aten::addmm        11.98%       2.402ms        18.16%       3.640ms      75.842us       3.563ms        17.24%       4.407ms      91.812us            48  \n",
      "    minimal_attn::mha_forward         2.58%     516.823us        18.07%       3.623ms     301.924us     675.000us         3.27%       3.695ms     307.917us            12  \n",
      "             aten::layer_norm         0.87%     173.578us        13.81%       2.769ms     110.770us     271.000us         1.31%       2.864ms     114.560us            25  \n",
      "      aten::native_layer_norm         6.52%       1.307ms        12.49%       2.503ms     100.137us       1.579ms         7.64%       2.593ms     103.720us            25  \n",
      "                  aten::empty         5.41%       1.085ms         6.80%       1.363ms       8.412us       1.960ms         9.48%       1.960ms      12.099us           162  \n",
      "                   aten::view         5.02%       1.007ms         5.02%       1.007ms       5.010us       1.698ms         8.22%       1.698ms       8.448us           201  \n",
      "                    aten::add         4.87%     977.210us         6.41%       1.285ms      26.217us       1.502ms         7.27%       1.502ms      30.653us            49  \n",
      "                     aten::to         0.86%     173.107us         6.82%       1.367ms      36.959us     309.000us         1.50%       1.497ms      40.459us            37  \n",
      "                    aten::mul         4.88%     978.971us         6.29%       1.260ms      26.250us       1.467ms         7.10%       1.467ms      30.562us            48  \n",
      "                aten::reshape         1.50%     301.330us         6.39%       1.280ms      47.412us     411.000us         1.99%       1.382ms      51.185us            27  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 20.047ms\n",
      "Self CUDA time total: 20.667ms\n",
      "\n",
      "Total time taken: 0.03205060958862305\n",
      "\n",
      "\n",
      "=== profiling `improved_mha_forward` attention === \n",
      "--------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                           aten::addmm        12.42%       2.457ms        19.43%       3.844ms      80.085us       3.628ms        17.77%       4.604ms      95.917us            48  \n",
      "    minimal_attn::improved_mha_forward         2.53%     499.590us        18.02%       3.564ms     297.010us     645.000us         3.16%       3.633ms     302.750us            12  \n",
      "                      aten::layer_norm         0.91%     180.414us        13.65%       2.699ms     107.963us     263.000us         1.29%       2.785ms     111.400us            25  \n",
      "               aten::native_layer_norm         6.60%       1.305ms        12.31%       2.435ms      97.386us       1.564ms         7.66%       2.522ms     100.880us            25  \n",
      "                           aten::empty         5.02%     992.650us         7.36%       1.455ms       8.982us       2.035ms         9.97%       2.035ms      12.562us           162  \n",
      "                            aten::view         4.67%     924.327us         4.67%     924.327us       4.599us       1.611ms         7.89%       1.611ms       8.015us           201  \n",
      "                              aten::to         0.88%     173.927us         6.85%       1.354ms      36.606us     296.000us         1.45%       1.480ms      40.000us            37  \n",
      "                             aten::add         4.72%     933.099us         6.23%       1.232ms      25.152us       1.438ms         7.04%       1.438ms      29.347us            49  \n",
      "                             aten::mul         4.64%     917.468us         6.06%       1.199ms      24.985us       1.408ms         6.90%       1.408ms      29.333us            48  \n",
      "                         aten::reshape         1.58%     311.945us         6.46%       1.279ms      47.354us     407.000us         1.99%       1.368ms      50.667us            27  \n",
      "--------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 19.779ms\n",
      "Self CUDA time total: 20.415ms\n",
      "\n",
      "Total time taken: 0.035830020904541016\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "attn_implementations = [\"sdpa\", \"mha_forward\", \"improved_mha_forward\"]\n",
    "for attn_implementation in attn_implementations:\n",
    "    print(f'=== profiling `{attn_implementation}` attention === ')\n",
    "    model.transformer.config._attn_implementation = attn_implementation\n",
    "    with torch.autograd.profiler.profile(use_device='cuda') as prof:\n",
    "        start_time = time.time()\n",
    "        out = model.forward(inputs['input_ids'])\n",
    "        end_time = time.time()\n",
    "    prof.export_chrome_trace(f\"traces/{attn_implementation}_trace.json\")  # note we can inspect these with chrome://tracing/\n",
    "    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10))\n",
    "    print(f\"Total time taken: {end_time - start_time}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f15b3-beb9-48d7-b3e0-c55b84212437",
   "metadata": {},
   "source": [
    "Note we see that the default minimal flash attn implementation is faster by ~7ms! And we can see the `aten::scaled_dot_product_attention` runs for the baseline and our `minimal_attn::mha_forward kernel` replaces it, as desired. For now, it does appear to take slightly more CUDA time, which makes sense as we have to use more memory -- the original version probably has more to do on the CPU. In the future we will test how that varies with different input sizes.\n",
    "\n",
    "| Name                                     | Self CPU %   | Self CPU    | CPU total % | CPU total   | CPU time avg | Self CUDA   | Self CUDA % | CUDA total  | CUDA time avg | # of Calls |\n",
    "| :--------------------------------------- | :----------- | :---------- | :---------- | :---------- | :----------- | :---------- | :---------- | :---------- | :------------ | :--------- |\n",
    "| `aten::scaled_dot_product_attention`     | 0.67%        | 213.412us   | 9.23%       | 2.931ms     | 244.211us    | 302.000us   | 1.27%       | 2.978ms     | 248.167us     | 12         |\n",
    "| `minimal_attn::mha_forward`              | 1.01%        | 162.292us   | 8.00%       | 1.285ms     | 107.096us    | 2.465ms     | 14.54%      | 3.323ms     | 276.917us     | 12         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d8705-2b39-424d-b971-99e835ca02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bunch of inputs with random tokens.\n",
    "# Note we are not doing real words bc we are not dealing with masking right now\n",
    "# and there should be no need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fa90c-2852-4118-a5c5-da0d69efbc85",
   "metadata": {},
   "source": [
    "### Generation\n",
    "Let's try generation now before starting the benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa01dd95-a45c-494a-b88f-4db7ac66c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def generate_and_decode(model, inputs, max_new_tokens=128, verbose=True):\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            inputs,\n",
    "            attention_mask=torch.ones_like(inputs),  # need else errors\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id  # need else errors\n",
    "        )\n",
    "        if verbose:\n",
    "            print(tokenizer.decode(output_ids[0]))\n",
    "\n",
    "def run_with_tracing(inputs, attn_implementation):\n",
    "    print(f'=== profiling `{attn_implementation}` attention === ')\n",
    "    model.transformer.config._attn_implementation = attn_implementation\n",
    "    with torch.autograd.profiler.profile(use_device='cuda') as prof:\n",
    "        start_time = time.time()\n",
    "        out = generate_and_decode(model, inputs)\n",
    "        end_time = time.time()\n",
    "    # prof.export_chrome_trace(f\"traces/{attn_implementation}_generation_trace.json\")  # note we can inspect these with chrome://tracing/\n",
    "    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10))\n",
    "    total_time_taken = end_time - start_time\n",
    "    print(f\"Total time taken: {total_time_taken}\\n\\n\")\n",
    "    return total_time_taken\n",
    "\n",
    "def run_with_no_profiling(inputs, attn_implementation):\n",
    "    model.transformer.config._attn_implementation = attn_implementation\n",
    "    start_time = time.time()\n",
    "    out = generate_and_decode(model, inputs, verbose=False)\n",
    "    end_time = time.time()\n",
    "    total_time_taken = end_time - start_time\n",
    "    return total_time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "302f8b92-46e1-4310-8fb4-f7afa4596711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== profiling `sdpa` attention === \n",
      "Today is the day when we can all be proud of our country and our values.\n",
      "\n",
      "We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We are all Americans. We\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        12.19%     286.732ms        17.92%     421.569ms      68.615us     454.704ms        18.63%     520.811ms      84.767us          6144  \n",
      "                                       aten::layer_norm         1.25%      29.432ms        14.24%     334.956ms     104.674us      41.985ms         1.72%     347.771ms     108.678us          3200  \n",
      "                     aten::scaled_dot_product_attention         1.07%      25.205ms        13.00%     305.762ms     199.064us      31.862ms         1.31%     312.323ms     203.335us          1536  \n",
      "                                aten::native_layer_norm         6.34%     149.132ms        12.51%     294.395ms      91.998us     177.063ms         7.26%     305.786ms      95.558us          3200  \n",
      "          aten::_scaled_dot_product_efficient_attention         2.44%      57.361ms        11.69%     274.944ms     179.000us      56.286ms         2.31%     280.461ms     182.592us          1536  \n",
      "                                            aten::empty         5.50%     129.499ms         5.62%     132.157ms       5.758us     221.183ms         9.06%     221.183ms       9.637us         22951  \n",
      "                                        aten::transpose         4.31%     101.322ms         6.84%     160.865ms      12.956us     141.838ms         5.81%     204.136ms      16.441us         12416  \n",
      "                                             aten::view         3.37%      79.285ms         3.37%      79.285ms       3.021us     176.571ms         7.24%     176.571ms       6.727us         26247  \n",
      "                                              aten::add         4.30%     101.087ms         5.94%     139.737ms      21.403us     168.812ms         6.92%     168.812ms      25.856us          6529  \n",
      "                                            aten::split         1.62%      38.180ms         6.78%     159.425ms     103.792us      41.960ms         1.72%     165.238ms     107.577us          1536  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.352s\n",
      "Self CUDA time total: 2.440s\n",
      "\n",
      "Total time taken: 3.7296695709228516\n",
      "\n",
      "\n",
      "=== profiling `mha_forward` attention === \n",
      "Today is the world's the more than the time, are more to the to the to the '-( to to the 'to the 'to to the 'to in to the 'to in to the 'to in the 'to the 'to in the 'to the 'd in at the 'ded to the 'd to the ' in the 'ded to the 'ded to the 'd in at the 'd, to the 'd in at the 'to the '(( to the level at the '(((( at ( at the '(to the, at the at the, at the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        11.03%     283.248ms        16.18%     415.595ms      67.642us     451.696ms        17.30%     515.212ms      83.856us          6144  \n",
      "                              minimal_attn::mha_forward         2.67%      68.467ms        18.28%     469.451ms     305.632us      85.976ms         3.29%     479.331ms     312.064us          1536  \n",
      "                                       aten::layer_norm         1.18%      30.178ms        13.11%     336.686ms     105.214us      42.657ms         1.63%     349.540ms     109.231us          3200  \n",
      "                                aten::native_layer_norm         5.89%     151.381ms        11.51%     295.566ms      92.364us     179.346ms         6.87%     306.883ms      95.901us          3200  \n",
      "                                               aten::to         1.03%      26.452ms         7.31%     187.631ms      34.837us      42.839ms         1.64%     199.491ms      37.039us          5386  \n",
      "                                            aten::empty         4.43%     113.665ms         4.43%     113.665ms       5.725us     190.463ms         7.30%     190.463ms       9.593us         19855  \n",
      "                                             aten::view         3.35%      86.049ms         3.35%      86.049ms       3.099us     188.727ms         7.23%     188.727ms       6.796us         27771  \n",
      "                                              aten::add         3.89%      99.894ms         5.41%     138.946ms      21.281us     168.000ms         6.44%     168.000ms      25.731us          6529  \n",
      "                                            aten::split         1.42%      36.454ms         6.11%     157.010ms     102.220us      41.840ms         1.60%     162.837ms     106.014us          1536  \n",
      "                                              aten::mul         3.76%      96.532ms         5.15%     132.302ms      20.662us     162.258ms         6.22%     162.258ms      25.341us          6403  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.568s\n",
      "Self CUDA time total: 2.611s\n",
      "\n",
      "Total time taken: 4.057413101196289\n",
      "\n",
      "\n",
      "=== profiling `improved_mha_forward` attention === \n",
      "Today is-S and the way, and the season, the first, the most, Edit\" - of's of of's…/ to the, the Edit\"'s the card of/ to, from the card of of's at the Edit Edit's of's of's of's of of's of of of's of's's, from the - of of of of? of at the team\" of of of's? of of at of of of of? of from the feel of of of's?/'s – the most of of of of of?\" of of from the team\"? of from the mood/gg\"\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::addmm        10.98%     284.383ms        16.10%     417.059ms      67.881us     452.450ms        17.12%     516.358ms      84.043us          6144  \n",
      "                     minimal_attn::improved_mha_forward         2.69%      69.759ms        18.39%     476.547ms     310.252us      85.739ms         3.24%     485.355ms     315.986us          1536  \n",
      "                                       aten::layer_norm         1.19%      30.733ms        13.11%     339.731ms     106.166us      43.212ms         1.64%     352.857ms     110.268us          3200  \n",
      "                                aten::native_layer_norm         5.91%     153.218ms        11.50%     298.001ms      93.125us     180.584ms         6.83%     309.645ms      96.764us          3200  \n",
      "                                               aten::to         1.05%      27.297ms         7.32%     189.719ms      35.224us      43.058ms         1.63%     202.018ms      37.508us          5386  \n",
      "                                            aten::empty         4.44%     115.068ms         4.44%     115.068ms       5.795us     192.897ms         7.30%     192.897ms       9.715us         19855  \n",
      "                                             aten::view         3.31%      85.829ms         3.31%      85.829ms       3.091us     190.456ms         7.21%     190.456ms       6.858us         27771  \n",
      "                                              aten::add         3.87%     100.230ms         5.36%     139.014ms      21.292us     168.518ms         6.38%     168.518ms      25.811us          6529  \n",
      "                                            aten::split         1.46%      37.738ms         6.19%     160.465ms     104.470us      42.507ms         1.61%     166.460ms     108.372us          1536  \n",
      "                                              aten::mul         3.75%      97.128ms         5.13%     132.910ms      20.757us     163.322ms         6.18%     163.322ms      25.507us          6403  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.591s\n",
      "Self CUDA time total: 2.643s\n",
      "\n",
      "Total time taken: 4.093547105789185\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\").to('cuda')['input_ids']\n",
    "attn_implementations = [\"sdpa\", \"mha_forward\", \"improved_mha_forward\"]\n",
    "for attn_implementation in attn_implementations:\n",
    "    run_with_tracing(inputs, attn_implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dddec8-2227-4257-8744-0dfc0e368f70",
   "metadata": {},
   "source": [
    "### Benchmarking\n",
    "For our full results, we will benchmark across a grid of experiments.\n",
    "\n",
    "Note that the GPT-2 context window is 1024 tokens, so the full input tokens + output tokens requested must reasonably sum to that.\n",
    "\n",
    "For performance, our sole focus is on speed, since Flash Attention results in the same output as attention so we cannot improve scores on language benchmarks.\n",
    "\n",
    "Hence our input will be all random tokens of various sizes, as the model itself doesn't see any difference between random tokens and sensible input.\n",
    "\n",
    "Our grid will be as follows:\n",
    "\n",
    "input sizes: [64, 128, 256, 512]\n",
    "\n",
    "max new tokens: [64, 128, 256, 512]\n",
    "\n",
    "We will run every combination on three methods: the original model, Minimal Flash Attention, and our Improved Minimal Flash Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c16be538-e7de-4538-b3ee-7a60ad061634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c1a787-a4d7-41ad-9813-e2d053897b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[8888,  318]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Today is\"], return_tensors=\"pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6406e3-1fe2-4de8-8604-5ad275ebc15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn: sdpa with Seq Len 64, Max New Tokens 64\n",
      "torch.Size([64, 64])\n",
      "\t1.7595970630645752s\n",
      "torch.Size([64, 64])\n",
      "\t1.7744083404541016s\n",
      "torch.Size([64, 64])\n",
      "\t1.7479560375213623s\n",
      "torch.Size([64, 64])\n",
      "\t1.7460238933563232s\n",
      "torch.Size([64, 64])\n",
      "\t1.7299389839172363s\n",
      "Attn: sdpa with Seq Len 64, Max New Tokens 128\n",
      "torch.Size([64, 64])\n",
      "\t1.7315726280212402s\n",
      "torch.Size([64, 64])\n",
      "\t1.729461908340454s\n",
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "inp_sizes = [64, 128, 256, 512]\n",
    "all_max_new_tokens = inp_sizes\n",
    "\n",
    "times = {}  # { attn_implementation: { input_output: [list of time taken] } }\n",
    "num_iterations = 5  # number of runs per attn_implementation, inp_size, max_new_tokens tuple\n",
    "batch_size = 64\n",
    "\n",
    "attn_implementations = [\"sdpa\", \"mha_forward\", \"improved_mha_forward\"]\n",
    "for attn_implementation in attn_implementations:\n",
    "    for inp_size in inp_sizes:\n",
    "        for max_new_tokens in all_max_new_tokens:\n",
    "            print(f\"Attn: {attn_implementation} with Seq Len {inp_size}, Max New Tokens {max_new_tokens}\")\n",
    "            for _ in range(num_iterations):\n",
    "                inputs = []\n",
    "                # Create random input\n",
    "                for _ in range(batch_size):\n",
    "                    input_tokens = []\n",
    "                    for _ in range(inp_size):\n",
    "                        input_tokens.append(random.randint(0, tokenizer.vocab_size - 1))\n",
    "                    input_tensor = torch.tensor(input_tokens, dtype=torch.int64).unsqueeze(0).to('cuda')  # unsqueeze since batch size is 1\n",
    "                    inputs.append(input_tensor)\n",
    "                inputs = torch.concatenate(inputs)\n",
    "                print(inputs.shape)\n",
    "\n",
    "                # Run\n",
    "                total_time_taken = run_with_no_profiling(inputs, attn_implementation)\n",
    "                print(f\"\\t{total_time_taken}s\")\n",
    "\n",
    "                # Store list of times\n",
    "                key = f'{inp_size}_{max_new_tokens}'\n",
    "                if attn_implementation not in times:\n",
    "                    times[attn_implementation] = {}\n",
    "                if key not in times[attn_implementation]:\n",
    "                    times[attn_implementation][key] = []\n",
    "                times[attn_implementation][key].append(total_time_taken)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8fae1c-be3f-400a-9434-7934ccc4cb85",
   "metadata": {},
   "source": [
    "Now we'll get summary statistics as well as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ab6f3ee1-01af-4c08-aa9a-db73f7d7f82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 64 with max new tokens of 64:\n",
      "mean=1.6331530809402466, median=1.6375927925109863, minimum=1.6183366775512695, maximum=1.6435298919677734, stddev=0.013170327991247177\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 64 with max new tokens of 128:\n",
      "mean=1.6122010946273804, median=1.6109435558319092, minimum=1.6104176044464111, maximum=1.6152417659759521, stddev=0.002646499779075384\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 64 with max new tokens of 256:\n",
      "mean=1.613917350769043, median=1.6134412288665771, minimum=1.6115765571594238, maximum=1.616734266281128, stddev=0.002611610572785139\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 64 with max new tokens of 512:\n",
      "mean=1.612900733947754, median=1.61283278465271, minimum=1.6127886772155762, maximum=1.6130805015563965, stddev=0.00015730576706118882\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 128 with max new tokens of 64:\n",
      "mean=1.6170543432235718, median=1.6167171001434326, minimum=1.6135404109954834, maximum=1.6209053993225098, stddev=0.003694055136293173\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 128 with max new tokens of 128:\n",
      "mean=1.6162470579147339, median=1.6158852577209473, minimum=1.6150333881378174, maximum=1.6178224086761475, stddev=0.001429269672371447\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 128 with max new tokens of 256:\n",
      "mean=1.6119918823242188, median=1.612579345703125, minimum=1.6070294380187988, maximum=1.6163666248321533, stddev=0.004696239717304707\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 128 with max new tokens of 512:\n",
      "mean=1.6095730066299438, median=1.608208179473877, minimum=1.6069467067718506, maximum=1.6135642528533936, stddev=0.003513562958687544\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 256 with max new tokens of 64:\n",
      "mean=1.6108890771865845, median=1.6120357513427734, minimum=1.6084606647491455, maximum=1.6121711730957031, stddev=0.002104259794577956\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 256 with max new tokens of 128:\n",
      "mean=1.6098181009292603, median=1.6087841987609863, minimum=1.6057260036468506, maximum=1.6149444580078125, stddev=0.004695409908890724\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 256 with max new tokens of 256:\n",
      "mean=1.6113108396530151, median=1.6107292175292969, minimum=1.6094393730163574, maximum=1.6137638092041016, stddev=0.0022201049141585827\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 256 with max new tokens of 512:\n",
      "mean=1.6104516983032227, median=1.6116695404052734, minimum=1.6057467460632324, maximum=1.6139390468597412, stddev=0.004229735117405653\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 512 with max new tokens of 64:\n",
      "mean=1.6106390953063965, median=1.6094396114349365, minimum=1.604907751083374, maximum=1.617569923400879, stddev=0.006415740121155977\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 512 with max new tokens of 128:\n",
      "mean=1.6092134714126587, median=1.6090235710144043, minimum=1.6089136600494385, maximum=1.6097033023834229, stddev=0.00042771684820763767\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 512 with max new tokens of 256:\n",
      "mean=1.6080135107040405, median=1.6087477207183838, minimum=1.6059198379516602, maximum=1.609372615814209, stddev=0.0018397957319393754\n",
      "--------------------------------------------------\n",
      "Running sdpa on sequence length of 512 with max new tokens of 512:\n",
      "mean=1.6092408895492554, median=1.6082000732421875, minimum=1.6057446002960205, maximum=1.6137776374816895, stddev=0.0041163950227200985\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 64 with max new tokens of 64:\n",
      "mean=1.7939900159835815, median=1.7962660789489746, minimum=1.789402723312378, maximum=1.7963011264801025, stddev=0.00397271616384387\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 64 with max new tokens of 128:\n",
      "mean=1.7817186117172241, median=1.7754786014556885, minimum=1.7728691101074219, maximum=1.7968077659606934, stddev=0.013132669031620026\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 64 with max new tokens of 256:\n",
      "mean=1.780366063117981, median=1.779205322265625, minimum=1.7767467498779297, maximum=1.7851462364196777, stddev=0.004318379797041416\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 64 with max new tokens of 512:\n",
      "mean=1.7785409688949585, median=1.7764534950256348, minimum=1.7762634754180908, maximum=1.7829058170318604, stddev=0.0037812974769622087\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 128 with max new tokens of 64:\n",
      "mean=1.7777191400527954, median=1.7773256301879883, minimum=1.7741429805755615, maximum=1.781689167022705, stddev=0.0037884614430367947\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 128 with max new tokens of 128:\n",
      "mean=1.776711344718933, median=1.7754724025726318, minimum=1.7745747566223145, maximum=1.7800867557525635, stddev=0.0029574809595942497\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 128 with max new tokens of 256:\n",
      "mean=1.7772287130355835, median=1.7770969867706299, minimum=1.7741549015045166, maximum=1.7804343700408936, stddev=0.0031418073922395706\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 128 with max new tokens of 512:\n",
      "mean=1.7699795961380005, median=1.7697155475616455, minimum=1.7677042484283447, maximum=1.7725191116333008, stddev=0.0024182708002626896\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 256 with max new tokens of 64:\n",
      "mean=1.780354619026184, median=1.775695562362671, minimum=1.770650863647461, maximum=1.79471755027771, stddev=0.012691797688603401\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 256 with max new tokens of 128:\n",
      "mean=1.7722591161727905, median=1.7724342346191406, minimum=1.7714207172393799, maximum=1.7729220390319824, stddev=0.0007658478571102023\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 256 with max new tokens of 256:\n",
      "mean=1.778922438621521, median=1.7812354564666748, minimum=1.7728431224822998, maximum=1.782688856124878, stddev=0.0053147925063967705\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 256 with max new tokens of 512:\n",
      "mean=1.7830638885498047, median=1.7775843143463135, minimum=1.7767095565795898, maximum=1.7948980331420898, stddev=0.010257929563522339\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 512 with max new tokens of 64:\n",
      "mean=1.7843025922775269, median=1.785247564315796, minimum=1.7774982452392578, maximum=1.7901618480682373, stddev=0.006384472828358412\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 512 with max new tokens of 128:\n",
      "mean=1.7739601135253906, median=1.7750158309936523, minimum=1.769016981124878, maximum=1.7778477668762207, stddev=0.004509043414145708\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 512 with max new tokens of 256:\n",
      "mean=1.7782407999038696, median=1.778038501739502, minimum=1.7771549224853516, maximum=1.779529094696045, stddev=0.0011999495327472687\n",
      "--------------------------------------------------\n",
      "Running mha_forward on sequence length of 512 with max new tokens of 512:\n",
      "mean=1.7724652290344238, median=1.7722461223602295, minimum=1.7717556953430176, maximum=1.7733938694000244, stddev=0.0008407790446653962\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 64 with max new tokens of 64:\n",
      "mean=1.7803163528442383, median=1.7806870937347412, minimum=1.778101921081543, maximum=1.7821598052978516, stddev=0.0020541998092085123\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 64 with max new tokens of 128:\n",
      "mean=1.7777103185653687, median=1.7782220840454102, minimum=1.7752280235290527, maximum=1.7796804904937744, stddev=0.0022699416149407625\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 64 with max new tokens of 256:\n",
      "mean=1.7771259546279907, median=1.7771801948547363, minimum=1.7735564708709717, maximum=1.7806413173675537, stddev=0.003542734310030937\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 64 with max new tokens of 512:\n",
      "mean=1.777086853981018, median=1.7759966850280762, minimum=1.775050401687622, maximum=1.7802135944366455, stddev=0.002748828148469329\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 128 with max new tokens of 64:\n",
      "mean=1.776740550994873, median=1.7771637439727783, minimum=1.774604082107544, maximum=1.7784538269042969, stddev=0.001959452172741294\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 128 with max new tokens of 128:\n",
      "mean=1.78449547290802, median=1.7845094203948975, minimum=1.7809183597564697, maximum=1.7880587577819824, stddev=0.0035702192690223455\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 128 with max new tokens of 256:\n",
      "mean=1.7799230813980103, median=1.7796883583068848, minimum=1.7783095836639404, maximum=1.7817714214324951, stddev=0.001742818276397884\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 128 with max new tokens of 512:\n",
      "mean=1.7814874649047852, median=1.780531883239746, minimum=1.7789061069488525, maximum=1.7850244045257568, stddev=0.0031691077165305614\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 256 with max new tokens of 64:\n",
      "mean=1.776477336883545, median=1.775674819946289, minimum=1.7735416889190674, maximum=1.7802155017852783, stddev=0.0034085141960531473\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 256 with max new tokens of 128:\n",
      "mean=1.7789472341537476, median=1.7798652648925781, minimum=1.7760913372039795, maximum=1.7808849811553955, stddev=0.002525251591578126\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 256 with max new tokens of 256:\n",
      "mean=1.775871753692627, median=1.774695873260498, minimum=1.7737462520599365, maximum=1.7791731357574463, stddev=0.002898238832131028\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 256 with max new tokens of 512:\n",
      "mean=1.7749475240707397, median=1.7748098373413086, minimum=1.7734065055847168, maximum=1.7766261100769043, stddev=0.0016142098465934396\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 512 with max new tokens of 64:\n",
      "mean=1.7758995294570923, median=1.7755346298217773, minimum=1.7736289501190186, maximum=1.7785353660583496, stddev=0.002473491244018078\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 512 with max new tokens of 128:\n",
      "mean=1.7784029245376587, median=1.7779381275177002, minimum=1.7743711471557617, maximum=1.7828998565673828, stddev=0.0042833201587200165\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 512 with max new tokens of 256:\n",
      "mean=1.7814174890518188, median=1.7713325023651123, minimum=1.771317720413208, maximum=1.8016023635864258, stddev=0.017480581998825073\n",
      "--------------------------------------------------\n",
      "Running improved_mha_forward on sequence length of 512 with max new tokens of 512:\n",
      "mean=1.7756147384643555, median=1.7749576568603516, minimum=1.7728004455566406, maximum=1.7790863513946533, stddev=0.003194064600393176\n"
     ]
    }
   ],
   "source": [
    "means = []\n",
    "for attn_implementation in attn_implementations:\n",
    "    for inp_size in inp_sizes:\n",
    "        for max_new_tokens in all_max_new_tokens:\n",
    "            key = f'{inp_size}_{max_new_tokens}'\n",
    "            if attn_implementation not in times or key not in times[attn_implementation]:\n",
    "                continue\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Running {attn_implementation} on sequence length of {inp_size} with max new tokens of {max_new_tokens}:\")\n",
    "            l = torch.Tensor(times[attn_implementation][key])\n",
    "            mean = l.mean().item()\n",
    "            median = l.median().item()\n",
    "            minimum = l.min().item()\n",
    "            maximum = l.max().item()\n",
    "            stddev = l.std().item()\n",
    "            print(f\"{mean=}, {median=}, {minimum=}, {maximum=}, {stddev=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
