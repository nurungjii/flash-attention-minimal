{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873bb9c9-5032-4d68-8163-b19a3ce09f4c",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First, load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c27d53-7f0b-4629-9c2d-ae84127512dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_gpt2 import GPT2Model\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=\"hf_home/\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", cache_dir=\"hf_home/\")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da24a7-6bd4-44bc-9eb1-14e3c8b0cf72",
   "metadata": {},
   "source": [
    "### Initial Testing\n",
    "Now, let's do forward passes on some sample inputs. Starting with the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5caef0-169b-4383-b8cf-6c416c20263c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0502,  0.0018, -0.1750,  ..., -0.1020, -0.0257, -0.1292],\n",
       "         [-0.2410, -0.0911,  0.2592,  ...,  0.4394,  0.3465,  0.1077]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\").to('cuda')\n",
    "model.config._attn_implementation = \"sdpa\"  # NOTE: This is default, but we set manually here for emphasis.\n",
    "out = model.forward(inputs['input_ids'])\n",
    "out.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82643ead-2b81-40c8-b2dc-12148c8f5a17",
   "metadata": {},
   "source": [
    "Now, using our attention implementation. Default is `sdpa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8428b93-1c78-4a6b-979d-4148903b8ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0562,  0.7767, -0.3577,  ..., -0.4087,  0.4106, -0.5051],\n",
       "         [-0.2669,  0.8194,  0.1462,  ..., -0.1800, -0.3680, -0.1513]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\").to('cuda')\n",
    "model.config._attn_implementation = \"minimal_attn\"\n",
    "out = model.forward(inputs['input_ids'])\n",
    "out.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ac93c-956d-4a9f-8a62-8b5162ef2800",
   "metadata": {},
   "source": [
    "Great! We can see the shapes are the same and the output tensors are too. This means the attention implementation is correct. Now, let's see if it is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3ddd08-c092-40e7-b497-48d946bac2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== profiling `sdpa` attention === \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      aten::addmm        11.59%       2.835ms        16.97%       4.151ms      86.476us       4.103ms        16.64%       4.884ms     101.750us            48  \n",
      "                                 aten::layer_norm         0.85%     207.728us        14.03%       3.432ms     137.271us     323.000us         1.31%       3.532ms     141.280us            25  \n",
      "                                 aten::contiguous         1.16%     283.709us        12.68%       3.103ms      86.191us     424.000us         1.72%       3.254ms      90.389us            36  \n",
      "                          aten::native_layer_norm         6.52%       1.595ms        12.64%       3.092ms     123.674us       1.973ms         8.00%       3.209ms     128.360us            25  \n",
      "               aten::scaled_dot_product_attention         0.95%     231.903us        11.99%       2.934ms     244.541us     275.000us         1.12%       2.982ms     248.500us            12  \n",
      "                                      aten::clone         2.77%     678.537us        10.85%       2.655ms      73.760us     889.000us         3.61%       2.830ms      78.611us            36  \n",
      "                                      aten::empty         6.70%       1.638ms         7.57%       1.852ms       8.821us       2.798ms        11.35%       2.798ms      13.324us           210  \n",
      "    aten::_scaled_dot_product_efficient_attention         2.34%     572.756us        10.81%       2.645ms     220.383us     663.000us         2.69%       2.707ms     225.583us            12  \n",
      "                                       aten::view         4.97%       1.215ms         4.97%       1.215ms       6.077us       2.147ms         8.71%       2.147ms      10.735us           200  \n",
      "                                  aten::transpose         3.89%     952.330us         6.54%       1.599ms      16.656us       1.390ms         5.64%       2.035ms      21.198us            96  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 24.464ms\n",
      "Self CUDA time total: 24.659ms\n",
      "\n",
      "=== profiling `minimal_attn` attention === \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  aten::addmm        13.60%       2.278ms        20.43%       3.423ms      71.313us       3.453ms        19.59%       4.193ms      87.354us            48  \n",
      "    minimal_attn::mha_forward         1.05%     175.259us         7.84%       1.314ms     109.468us       2.456ms        13.93%       3.327ms     277.250us            12  \n",
      "             aten::layer_norm         0.95%     158.591us        15.64%       2.620ms     104.808us     270.000us         1.53%       2.703ms     108.120us            25  \n",
      "      aten::native_layer_norm         6.90%       1.156ms        14.00%       2.345ms      93.803us       1.442ms         8.18%       2.433ms      97.320us            25  \n",
      "                  aten::empty         5.51%     922.698us         7.01%       1.174ms       8.508us       1.525ms         8.65%       1.525ms      11.051us           138  \n",
      "                    aten::add         5.27%     882.589us         6.96%       1.166ms      23.805us       1.370ms         7.77%       1.370ms      27.959us            49  \n",
      "                    aten::mul         5.29%     885.469us         6.90%       1.156ms      24.091us       1.356ms         7.69%       1.356ms      28.250us            48  \n",
      "                   aten::view         5.26%     880.974us         5.26%     880.974us       4.405us       1.324ms         7.51%       1.324ms       6.620us           200  \n",
      "                  aten::split         1.64%     275.501us         7.16%       1.200ms      99.984us     351.000us         1.99%       1.245ms     103.750us            12  \n",
      "                 aten::narrow         1.37%     229.013us         4.67%     782.251us      21.729us     368.000us         2.09%     894.000us      24.833us            36  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 16.754ms\n",
      "Self CUDA time total: 17.629ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "attn_implementations = [\"sdpa\", \"minimal_attn\"]\n",
    "for attn_implementation in attn_implementations:\n",
    "    print(f'=== profiling `{attn_implementation}` attention === ')\n",
    "    model.config._attn_implementation = attn_implementation\n",
    "    with torch.autograd.profiler.profile(use_device='cuda') as prof:\n",
    "        out = model.forward(inputs['input_ids'])\n",
    "    print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fa90c-2852-4118-a5c5-da0d69efbc85",
   "metadata": {},
   "source": [
    "### Generation\n",
    "Let's try generation now before starting the benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01dd95-a45c-494a-b88f-4db7ac66c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: No generation for now bc some way the model works. Probably need to set up with AutoModel but can't figure out for now how to do that and retain our version of modeling_gpt2.py\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=5,\n",
    "#     return_dict_in_generate=True,\n",
    "#     output_scores=True,\n",
    "#     do_sample=False,  # temperature = 0.0 so deterministic\n",
    "# )\n",
    "\n",
    "# May need for batching\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
